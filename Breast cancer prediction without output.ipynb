{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb006a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bcbf6",
   "metadata": {},
   "source": [
    "# Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa44fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score , classification_report,ConfusionMatrixDisplay,precision_score,recall_score, f1_score,roc_auc_score,roc_curve, balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "import tensorflow as tf \n",
    "tf.random.set_seed(3)\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bee262",
   "metadata": {},
   "source": [
    "# The dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data=pd.read_csv(\"Diabetes_dataset.csv\") #loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bed50",
   "metadata": {},
   "source": [
    "# Data Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diabetes_data.head() #printing first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f608cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.shape #checking the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data['Diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.describe() #descriptive statistic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.isna().sum() #Checking the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.isnull().sum() #checking the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe04e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.info() #getting the datatypes info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9461914",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.duplicated().sum() #finding the duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b3a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if diabetes_data.duplicated().sum() == 0:\n",
    "        print('No Duplicated Values')\n",
    "else:\n",
    "    print('Duplicated data has been eliminated')\n",
    "    diabetes_data.drop_duplicates()   #eliminating the duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6241a9",
   "metadata": {},
   "source": [
    "# EDA & Data visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "counts = diabetes_data['Diabetes'].value_counts()\n",
    "plt.figure(figsize=[7,5])\n",
    "plt.pie(counts, labels=counts.index, autopct='%1.1f%%',textprops=dict(color=\"azure\"),colors=['darkblue', 'deepskyblue'])\n",
    "plt.title('Target distribution in the dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aaaf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplot to find correlation btn all features:\n",
    "sns.pairplot(diabetes_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting variables that are likely to predict diabetes medically:\n",
    "#dia_data = diabetes_data[[\"Age\",\"Sex\",\"HighChol\",\"BMI\",\"Smoker\",\"PhysActivity\",\"PhysHlth\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\"GenHlth\",\"Stroke\",\"HighBP\",\"Diabetes\"]]\n",
    "#dia_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990d69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking unique value count:\n",
    "unique_values = {}\n",
    "for col in diabetes_data.columns:\n",
    "    unique_values[col] = diabetes_data[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency check for all values in the column:\n",
    "\n",
    "# All data columns except for color\n",
    "feature_cols = [x for x in diabetes_data.columns]\n",
    "plt.figure(figsize=(25,35))\n",
    "# loop for subplots\n",
    "for i in range(len(feature_cols)):\n",
    "    plt.subplot(8,5,i+1)\n",
    "    plt.title(feature_cols[i])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.hist(diabetes_data[feature_cols[i]],color = \"darkblue\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f065f",
   "metadata": {},
   "source": [
    "Now, dropping columns with very small value range:'HvyAlcoholConsump' and 'stroke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5aca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.drop(['HvyAlcoholConsump','Stroke'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation of other features with Diabetes:\n",
    "diabetes_data.drop('Diabetes', axis=1).corrwith(diabetes_data.Diabetes).plot(kind='bar', grid=True, figsize=(10, 6), title=\"Correlation with Diabetes\",color=\"darkblue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ad2f6",
   "metadata": {},
   "source": [
    "Findings:variables with correlation less than 0.1 are Sex, Smoker, Fruits, Veggies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58df9b",
   "metadata": {},
   "source": [
    "# Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfa8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for all possible co-variates:\n",
    "sns.set(rc = {'figure.figsize':(10,10)})\n",
    "sns.heatmap(diabetes_data.corr(),vmin=-1, vmax=1, annot = True, fmt='.1g',cmap= 'YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the variables with low correlations: \n",
    "diabetes_data.drop(['Sex','Fruits'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956a48a",
   "metadata": {},
   "source": [
    "#narrowed down to 13 possible determinants \n",
    "#determine which predictors are more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate bar plot for categorical variables\n",
    "features = [x for x in diabetes_data.columns if x not in ['Age', 'BMI', 'PhysHlth', 'Diabetes']]\n",
    "plt.figure(figsize=(42, 24))  \n",
    "plt.suptitle('Diabetes by categorical features',fontsize=24)\n",
    "\n",
    "# Subplots\n",
    "for i in enumerate(features):\n",
    "    plt.subplot(4, 4, i[0] + 1)\n",
    "    x = sns.countplot(\n",
    "        data=diabetes_data,\n",
    "        x=i[1],\n",
    "        hue='Diabetes',\n",
    "        palette=['darkblue', 'red']\n",
    "    )\n",
    "    for z in x.patches:\n",
    "        x.annotate(\n",
    "            '{:.1f}'.format((z.get_height() / diabetes_data.shape[0]) * 100) + '%',\n",
    "            (z.get_x() + 0.25, z.get_height() + 0.01)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb32fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for numeric variables\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.displot(x='BMI', col='Diabetes' , data = diabetes_data, color = 'darkblue')\n",
    "sns.displot(data=diabetes_data,col='Diabetes',x='Age', color='darkblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dffb26",
   "metadata": {},
   "source": [
    "# Feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features selection -step 1\n",
    "#1. Define X,y\n",
    "Y = (diabetes_data['Diabetes']).astype(int)\n",
    "X = diabetes_data.loc[:, diabetes_data.columns != 'Diabetes']  # everything except \"Diabetes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "print(model.feature_importances_) \n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "plt.figure(figsize=(8,6))\n",
    "feat_importances.nlargest(6).plot(kind='barh',color='darkblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04555dd",
   "metadata": {},
   "source": [
    "# Splitting data into train data & test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (diabetes_data['Diabetes']).astype(int)\n",
    "X = diabetes_data.loc[:, diabetes_data.columns != 'stroke']  # everything except \"stroke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "S= StandardScaler() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d942114",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.fit(X) #standardising the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stdz_data=S.transform(X) #transforming the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bce6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Stdz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c99cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b99c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(Stdz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64243ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca=pca.transform(Stdz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stdz_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab771364",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stdz_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d71959",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f43ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= diabetes_data['Diabetes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6558bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size= 0.4)\n",
    "#, stratify=Y,random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3551b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape,X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b689ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca20312",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_train = lda.fit_transform(X_train, Y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778944f9",
   "metadata": {},
   "source": [
    "# Model building and testing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9910f5c",
   "metadata": {},
   "source": [
    "# KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872946cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn = knn.predict(X_test)\n",
    "pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_modelX1(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_knn # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,pred_knn ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a62506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default KNN model\n",
    "accuracy_knn = accuracy_score(Y_test, pred_knn)\n",
    "print(\"Accuracy score for default KNN model:\", accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = train_evaluate_modelX1(Y_test)\n",
    "R1.index = ['K Nearest Neighbors - Method 1']\n",
    "R1.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59df84c",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for K Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd43b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# defining parameter range\n",
    "param_grid = {'n_neighbors': [1,3,5,7,9,11,13,15,17,19],  #odd numbers because there are 2 classes in target coulmn\n",
    "              'weights': ['distance', 'uniform']}  \n",
    "gridKNN = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridKNN.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fe45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridKNN.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc50ba1",
   "metadata": {},
   "source": [
    "# KNN with best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors = 19, weights= 'distance')\n",
    "knn2.fit(X_train, Y_train)\n",
    "pred_knn2 = knn2.predict(X_test)\n",
    "print(classification_report(Y_test, pred_knn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d5614",
   "metadata": {},
   "source": [
    "# Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Trim the shorter array\n",
    "Y_test = Y_test[:len(pred_knn2)]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_test, pred_knn2)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Remove the cell gridlines\n",
    "plt.grid(which='major')\n",
    "\n",
    "# Set the figure size\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99939a5",
   "metadata": {},
   "source": [
    "# Accuracy score for KNN with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f415ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_knn2 = accuracy_score(Y_test, pred_knn2)\n",
    "print(\"Accuracy score for KNN model with best parameters:\", accuracy_knn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58af3e",
   "metadata": {},
   "source": [
    "# Cross-validation using scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5460039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = 19, weights= 'distance')\n",
    "    knn2.fit(X_train, Y_train)   \n",
    "    predictions = knn2.predict(X_test)\n",
    "    print(classification_report(predictions, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_knn3 = accuracy_score(Y_test, predictions)\n",
    "print(\"Accuracy score for KNN model with best parameters:\", accuracy_knn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6531ec2",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_model1(Y_test):\n",
    "    #fit the model instance \n",
    "    #predictions = pred_knn3 # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_evaluate_model1(Y_test)\n",
    "results.index = ['K Nearest Neighbors - Method 1']\n",
    "results.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store the metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Value': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Create a bar chart to visualize the metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Metric'], metrics_df['Value'],color='darkblue')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evaluation Metrics for K-Nearest Neighbors')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e688f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4dbf2",
   "metadata": {},
   "source": [
    "# Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier()\n",
    "RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_RF = RF.predict(X_test)\n",
    "pred_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28402d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default rf model\n",
    "accuracy_RF = accuracy_score(Y_test, pred_RF)\n",
    "print(\"Accuracy score for default Random Forest model:\", accuracy_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_model2(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_RF # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865e9f0",
   "metadata": {},
   "source": [
    "# Tuning for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# defining parameter range\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],   \n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "gridrf = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 5)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridrf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86646875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridrf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d16fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's run our SVC again with the best parameters.\n",
    "rf2 = RandomForestClassifier(criterion='entropy', max_features='log2', n_estimators= 10)\n",
    "rf2.fit(X_train, Y_train)\n",
    "pred_rf2 = rf2.predict(X_test)\n",
    "print(classification_report(Y_test, pred_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rf2 = accuracy_score(Y_test,pred_rf2)\n",
    "print(\"Accuracy score for RF model with best parameters:\", accuracy_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsRF = train_evaluate_model2(Y_test)\n",
    "resultsRF.index = ['Random Forest - Method 2']\n",
    "results= results.append(resultsRF)\n",
    "resultsRF.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the shorter array\n",
    "Y_test = Y_test[:len(pred_rf2)]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_test, pred_rf2)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Remove the cell gridlines\n",
    "plt.grid(which='major')\n",
    "\n",
    "# Set the figure size\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20635cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store the metrics\n",
    "accuracy = accuracy_score(Y_test, pred_rf2 )\n",
    "f1 = f1_score(Y_test, pred_rf2 )\n",
    "precision = precision_score(Y_test, pred_rf2 )\n",
    "recall = recall_score(Y_test, pred_rf2 )\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Value': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Create a bar chart to visualize the metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Metric'], metrics_df['Value'],color='darkblue')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evaluation Metrics for Random Forest')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72443f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, pred_rf2)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(Y_test, pred_rf2)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435ec06",
   "metadata": {},
   "source": [
    "# Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8da237",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB=GaussianNB()\n",
    "GNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GNB = GNB.predict(X_test)\n",
    "pred_GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43861cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_modelX3(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_GNB # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred_GNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc313e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default Gaussian Naive Bayes model\n",
    "accuracy_GNB = accuracy_score(Y_test, pred_GNB)\n",
    "print(\"Accuracy score for default Gaussian Naive Bayes model:\", accuracy_GNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R3 = train_evaluate_modelX3(Y_test)\n",
    "R3.index = ['Gaussian Naive Bayes - Method 3']\n",
    "R1= R1.append(R3)\n",
    "R3.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1106958",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97567b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# defining parameter range\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "gridGNB = GridSearchCV(GaussianNB(), param_grid, refit = True, verbose = 5)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridGNB.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridGNB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dffbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's run our SVC again with the best parameters.\n",
    "GNB2 = GaussianNB(var_smoothing = 0.2848035868435802)\n",
    "GNB2.fit(X_train, Y_train)\n",
    "pred_GNB2 = GNB2.predict(X_test)\n",
    "print(classification_report(Y_test, pred_GNB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_GNB2 = accuracy_score(Y_test,pred_GNB2)\n",
    "print(\"Accuracy score for nb model with best parameters:\", accuracy_GNB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the shorter array\n",
    "Y_test = Y_test[:len(pred_GNB2)]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_test, pred_GNB2)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Remove the cell gridlines\n",
    "plt.grid(which='major')\n",
    "\n",
    "# Set the figure size\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_model3(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_GNB2 # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a937de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsGNB = train_evaluate_model3(Y_test)\n",
    "resultsGNB.index = ['Gaussian Naive Bayes - Method 3']\n",
    "results= results.append(resultsGNB)\n",
    "resultsGNB.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce59ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store the metrics\n",
    "predictions = pred_GNB2\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Value': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Create a bar chart to visualize the metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Metric'], metrics_df['Value'],color='darkblue')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evaluation Metrics for Naive Bayes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a830ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "predictions = pred_GNB2\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b710249",
   "metadata": {},
   "source": [
    "# SVM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = svc.predict(X_test)\n",
    "pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default SVC model\n",
    "accuracy_svc = accuracy_score(Y_test, pred_svc)\n",
    "print(\"Accuracy score for default SVC model:\", accuracy_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77409be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_model4(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_svc # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultssvc = train_evaluate_model4(Y_test)\n",
    "resultssvc.index = ['Support Vector Machine - Method 4']\n",
    "results= results.append(resultssvc)\n",
    "resultssvc.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the shorter array\n",
    "Y_test = Y_test[:len(pred_svc)]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_test, pred_svc)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Remove the cell gridlines\n",
    "plt.grid(which='major')\n",
    "\n",
    "# Set the figure size\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71685bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store the metrics\n",
    "predictions=pred_svc\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Value': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Create a bar chart to visualize the metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Metric'], metrics_df['Value'])\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evaluation Metrics for SVM')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "predictions = pred_svc\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79dbdc",
   "metadata": {},
   "source": [
    "# Tuning for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa31af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "param_grid = [\n",
    "    {'C': [1, 10,], 'kernel': ['linear']},\n",
    "    {'gamma': [0.001, 0.0001]},\n",
    "]\n",
    "\n",
    " \n",
    "gridsvc = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridsvc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridsvc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our SVC again with the best parameters.\n",
    "svc2 = svm.SVC( gamma= 0.001)\n",
    "svc2.fit(X_train, Y_train)\n",
    "pred_svc2 = svc2.predict(X_test)\n",
    "print(classification_report(Y_test, pred_svc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8791e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default SVC model\n",
    "accuracy_svc2 = accuracy_score(Y_test, pred_svc2)\n",
    "print(\"Accuracy score for default SVC model:\", accuracy_svc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    svc2 = KNeighborsClassifier(n_neighbors = 19, weights= 'distance')\n",
    "    svc2.fit(X_train, Y_train)   \n",
    "    predictions = svc2.predict(X_test)\n",
    "    print(classification_report(predictions, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svc3 = accuracy_score(Y_test, predictions)\n",
    "print(\"Accuracy score for Logistic regresssion model with best parameters:\", accuracy_svc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66148490",
   "metadata": {},
   "source": [
    "# Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "log= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log = log.predict(X_test)\n",
    "pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_modelX5(Y_test):\n",
    "    #fit the model instance \n",
    "    predictions = pred_log # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2280473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score for the default Logistic regresssion model\n",
    "accuracy_log = accuracy_score(Y_test, pred_log)\n",
    "print(\"Accuracy score for default Logistic regresssion model:\", accuracy_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15698f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "R5 = train_evaluate_modelX5(Y_test)\n",
    "R5.index = ['Gaussian Naive Bayes - Method 3']\n",
    "R1= R1.append(R5)\n",
    "R5.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1c320",
   "metadata": {},
   "source": [
    "# Turning for Logistic regresssion using GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47539d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "               'solver': ['lbfgs', 'sag', 'newton-cg']}\n",
    "\n",
    " \n",
    "gridlog = GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "gridlog.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridlog.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our SVC again with the best parameters.\n",
    "log2 = LogisticRegression(C= 10,\n",
    "    solver= 'lbfgs',)\n",
    "log2.fit(X_train, Y_train)\n",
    "pred_log2 = log2.predict(X_test)\n",
    "print(classification_report(Y_test, pred_log2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31fcb3b",
   "metadata": {},
   "source": [
    "# Cross-validation using scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4dcf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    log2 = LogisticRegression(C= 1,\n",
    "    solver= 'lbfgs',)\n",
    "    log2.fit(X_train, Y_train)  \n",
    "    predictions = knn2.predict(X_test)\n",
    "    print(classification_report(predictions, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f020e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_log3 = accuracy_score(Y_test, predictions)\n",
    "print(\"Accuracy score for Logistic regresssion model with best parameters:\", accuracy_log3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3705c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get y_test and calculate into df all the relevant metric\n",
    "def train_evaluate_model5(Y_test):\n",
    "    #fit the model instance \n",
    "    #predictions = pred_log # calculate predictions\n",
    "\n",
    "    #compute metrics for evaluation\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    f1 = f1_score(Y_test, predictions)\n",
    "    precision = precision_score(Y_test, predictions)\n",
    "    recall = recall_score(Y_test, predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(Y_test, predictions)\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "    #create a dataframe to visualize the results\n",
    "    eval_df = pd.DataFrame([[accuracy, f1, precision, recall, balanced_accuracy, auc]], columns=['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'auc'])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultslog = train_evaluate_model5(Y_test)\n",
    "resultslog.index = ['Logistic regresssion - Method 5']\n",
    "results= results.append(resultslog)\n",
    "resultslog.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the shorter array\n",
    "Y_test = Y_test[:len(predictions)]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Remove the cell gridlines\n",
    "plt.grid(which='major')\n",
    "\n",
    "# Set the figure size\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store the metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Value': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Create a bar chart to visualize the metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_df['Metric'], metrics_df['Value'])\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Evaluation Metrics for Logistic regresssion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff74595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb1c18",
   "metadata": {},
   "source": [
    "# Predictive system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =diabetes_data.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data['Diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size= 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# assuming you have your X and Y DataFrames here\n",
    "\n",
    "# define the number of folds for k-fold cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# iterate over each fold in the k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # use numpy.r_ to convert the index arrays into arrays of column indices\n",
    "    X_train, X_test = X.iloc[np.r_[train_index]], X.iloc[np.r_[test_index]]\n",
    "    Y_train, Y_test = Y.iloc[np.r_[train_index]], Y.iloc[np.r_[test_index]]\n",
    "    \n",
    "    # create an instance of the LogisticRegression model\n",
    "    log2 = LogisticRegression(C= 1, penalty='l1', solver='liblinear')\n",
    "    \n",
    "    # train the model on the training data\n",
    "    log2.fit(X_train, Y_train)\n",
    "    \n",
    "    # test the model on the test data\n",
    "    Y_pred = log2.predict(X_test)\n",
    "    \n",
    "    # print the classification report and accuracy score for this fold\n",
    "    print(\"\\nClassification report for logistic regression model in this fold:\")\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "    accuracy_log2 = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Accuracy score for logistic regression model in this fold:\", accuracy_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f93ad1",
   "metadata": {},
   "source": [
    "# Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # accuracy score on the training data\n",
    "X_train_prediction = log2.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e52ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score of the training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f3f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score on the test data\n",
    "\n",
    "test_data_accuracy = accuracy_score(predictions, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b54a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score of the test data : ', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ead6a9",
   "metadata": {},
   "source": [
    "# Making a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bec2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = (6,1,1,37,0,0,0,1,4,0,0,0,0)\n",
    "\n",
    "# changing the input_data to numpy array\n",
    "input_data_as_numpy_array = np.asarray(input_data)\n",
    "\n",
    "# reshape the array as we are predicting for one instance\n",
    "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
    "\n",
    "predictions = log2.predict(input_data_reshaped)\n",
    "print(predictions)\n",
    "\n",
    "if (predictions[0] == 0):\n",
    "  print('The person is not diabetic')\n",
    "else:\n",
    "  print('The person is diabetic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d14a4",
   "metadata": {},
   "source": [
    "# Model Comparison after tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f0673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.style.background_gradient(cmap = sns.color_palette(\"blend:darkblue,deepskyblue\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0ad78",
   "metadata": {},
   "source": [
    "# ML Comparison graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d764bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5)) \n",
    "\n",
    "labels = [\"KNN\", \"RF\",\"GNB\",\"SVM\",\"LG\"]\n",
    "accuracy_values = [95.01,95.15,84.94,98.86,98.99]\n",
    "\n",
    "plt.bar(labels,accuracy_values,color='darkblue')\n",
    "\n",
    "for i,v in enumerate(accuracy_values):\n",
    "    plt.text(i, v/2, str(v), ha='center', color='gold', fontsize=20,fontweight='bold')\n",
    "\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparison of ML models\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18417583",
   "metadata": {},
   "source": [
    "# Deep learning techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e366d",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.random.set_seed(3)\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1558065",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\") #loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a 3D tensor for CNN input\n",
    "X = np.array(X).reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = keras.Sequential()\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(BatchNormalization())\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, accuracy = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3549fb",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) Using LSTM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e304745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "model = keras.Sequential()\n",
    "#model.add(keras.layers.Embedding(input_dim=1000, output_dim=64))\n",
    "model.add(keras.layers.LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(keras.layers.LSTM(units=32))\n",
    "model.add(keras.layers.Dense(128, activation='relu'),)\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c66ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96131f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, accuracy2 = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691de8e",
   "metadata": {},
   "source": [
    "# Multilayer perceptron(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\")\n",
    "\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler= StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "# Define the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(18, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_std, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Mean Absolute Error: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3153959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, accuracy2 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36360ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "y_proba = model.predict(X_test)[:, 0]\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e7cca",
   "metadata": {},
   "source": [
    "# Bayesian Optimized Long-Short Term Memory Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e87992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data_2=pd.read_csv(\"Diabetes_dataset.csv\") #loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3181892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "X =diabetes_data_2.drop(columns='Diabetes',axis=1)\n",
    "Y = diabetes_data_2['Diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S= StandardScaler() \n",
    "S.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcf277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stdz_data=S.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(Stdz_data)\n",
    "x_pca=pca.transform(Stdz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be48326",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_pca\n",
    "y= diabetes_data['Diabetes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd91a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, accuracy3 = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predict the output for the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean absolute error and mean squared error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568837f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "y_proba = model.predict(X_test)[:, 0]\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='(AUC = %0.2f)' % auc)\n",
    "\n",
    "# Print the AUC\n",
    "print('AUC:', auc)\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\")\n",
    "print('False Positive Rates:', fpr)\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print('True Positive Rates:', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f8395",
   "metadata": {},
   "source": [
    "# Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6)) \n",
    "\n",
    "labels = [\"CNN\", \"MLP\",\"RNN\"]\n",
    "accuracy_values = [accuracy, accuracy2, accuracy3]\n",
    "r_accuracy_values = [round(v, 2) for v in accuracy_values]\n",
    "plt.bar(labels,r_accuracy_values)\n",
    "\n",
    "for i,v in enumerate(r_accuracy_values):\n",
    "    plt.text(i, v/2, str(v), ha='center', color='red', fontsize=20,fontweight='bold')\n",
    "\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy of Different Algorithms\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "\n",
    "labels = [\"KNN\", \"RF\",\"GNB\",\"SVM\",\"LG\",\"CNN\", \"MLP\",\"RNN\"]\n",
    "accuracy_values = [95.01,95.15,84.94,98.86,98.99,accuracy*100, accuracy2*100, accuracy3*100]\n",
    "r_accuracy_values = [round(v, 2) for v in accuracy_values]\n",
    "\n",
    "# Define a list of colors for each bar\n",
    "colors = ['darkblue'] * len(labels)\n",
    "colors[-3:] = ['deepskyblue','deepskyblue','deepskyblue'] # Change the last 3 colors to red, green, and blue\n",
    "\n",
    "# Plot the bars with the specified colors\n",
    "plt.bar(labels, r_accuracy_values, color=colors)\n",
    "\n",
    "# Annotate the bars with their accuracy values\n",
    "for i, v in enumerate(r_accuracy_values):\n",
    "    plt.text(i, v/2, str(v), ha='center', color='azure', fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy comparison between ML and Deep learning\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e4417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
